{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60902189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "affbd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6da9350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12386a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff65941",
   "metadata": {},
   "source": [
    "### Cell type classification using single cell RNA seq profiles\n",
    "\n",
    "For this notebook, I will use the retinal bipolar cells dataset from the visualization notebook and explore how well expression profiles can predict the cell type (the ground truth for which is obtained from expert annotation by the authors) using a neural network implementation in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4952784",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle(\"/Users/anuraglimdi/Desktop/Single_cell_workshop/Datasets/retinal_bipolar/retinal_bipolar_data.pickle.gz\")\n",
    "metadata = pd.read_pickle(\"/Users/anuraglimdi/Desktop/Single_cell_workshop/Datasets/retinal_bipolar/retinal_bipolar_metadata.pickle.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc2e5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21552, 15524)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c69703",
   "metadata": {},
   "source": [
    "This is too much data to run on a local computer, going to reduce this data to fewer dimensions using PCA and then work with the first 50 or 100 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5e93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the scprep helper functions\n",
    "data_pca = scprep.reduce.pca(data_raw, n_components=50, method='dense').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268193cb",
   "metadata": {},
   "source": [
    "Standardizing the data (by column) with mean = 0 and variance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ddd9256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  #instantiating the standard scalar class\n",
    "data_scaled = scaler.fit_transform(data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "433d2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert cell type labels to numbers using pandas\n",
    "labels, cluster_names = pd.factorize(metadata['CELLTYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c2632",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training/test sets\n",
    "\n",
    "Using the train_test_split function; note that this function is incredibly slow if not using the PCA reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e5741c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_train, expr_test, cell_train, cell_test = train_test_split(data_scaled, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "04dd0fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17241, 50)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981214d",
   "metadata": {},
   "source": [
    "### Building the neural network using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b3884",
   "metadata": {},
   "source": [
    "Defining the dataset class for the single cell RNA seq input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8ae8de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scRNAseq_Dataset(Dataset):\n",
    "    def __init__(self, expression, labels):\n",
    "        self.labels = labels     # cell type labels\n",
    "        self.expression = expression   # PCA reduced and scaled expression matrix\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]     # label for the idx'th cell type\n",
    "        expression = self.expression[idx].astype('float') #a vector of expression PCA components for each cells\n",
    "        \n",
    "        return expression, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955fa04",
   "metadata": {},
   "source": [
    "Defining the neural network itself:\n",
    "\n",
    "Including two fully connected layers following by a log(softmax) output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "dd60cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_components):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_components, 128) #dense (fully connected) layer going from input_components to 128\n",
    "        self.activation = nn.ReLU()  # rectified linear unit activation function\n",
    "        self.linear2 = nn.Linear(128, 50) #another dense layer (128 to 50)\n",
    "\n",
    "    def forward(self, x):   #defining the forward propagation through the network\n",
    "        x = self.linear1(x)    \n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        output = F.log_softmax(x, dim=-1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = NeuralNetwork(input_components = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e39f6a",
   "metadata": {},
   "source": [
    "Instantiation of the `Dataset` class using the split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b42fb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = scRNAseq_Dataset(expression=expr_train, labels=cell_train)\n",
    "test_data = scRNAseq_Dataset(expression=expr_test, labels=cell_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9533d0",
   "metadata": {},
   "source": [
    "Using the `DataLoader` class to get the iterable for batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "72ec55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce1bad",
   "metadata": {},
   "source": [
    "Loss function is negative log-likelihood (note that this is the appropriate function given that the model outputs log probabilities; if the output was labels, then the loss function would be a cross entropy function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5e613086",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()   #negative log-likelihood loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560524b",
   "metadata": {},
   "source": [
    "Setting parameters for the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "25972c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa9a0e",
   "metadata": {},
   "source": [
    "The optimizer uses stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "948eb929",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2328eb",
   "metadata": {},
   "source": [
    "Defining functions for the training and test loop for the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "862f3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):    \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.float())    #convert input to type float otherwise forward propagation fails\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()   #count how many predictions are correct\n",
    "            # this works by computing how many times the model prediction of highest log(probability) matches\n",
    "            # the true label\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "fee6f248",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.908354  [    0/17241]\n",
      "loss: 1.048406  [ 6400/17241]\n",
      "loss: 0.758581  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.717485 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.721321  [    0/17241]\n",
      "loss: 0.520383  [ 6400/17241]\n",
      "loss: 0.685628  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.707738 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.669907  [    0/17241]\n",
      "loss: 0.795179  [ 6400/17241]\n",
      "loss: 0.647724  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.698879 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.829800  [    0/17241]\n",
      "loss: 0.754176  [ 6400/17241]\n",
      "loss: 0.601442  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.690395 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.832526  [    0/17241]\n",
      "loss: 0.771664  [ 6400/17241]\n",
      "loss: 0.669948  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.677487 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.578678  [    0/17241]\n",
      "loss: 0.799996  [ 6400/17241]\n",
      "loss: 0.644747  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.666688 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.867755  [    0/17241]\n",
      "loss: 0.929421  [ 6400/17241]\n",
      "loss: 0.564468  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.656455 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.893159  [    0/17241]\n",
      "loss: 0.626076  [ 6400/17241]\n",
      "loss: 0.446982  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.649586 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.588652  [    0/17241]\n",
      "loss: 0.534901  [ 6400/17241]\n",
      "loss: 0.513811  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.642752 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.580061  [    0/17241]\n",
      "loss: 0.868675  [ 6400/17241]\n",
      "loss: 0.659857  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.631448 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.668128  [    0/17241]\n",
      "loss: 0.640402  [ 6400/17241]\n",
      "loss: 0.448146  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.624257 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.564316  [    0/17241]\n",
      "loss: 0.822690  [ 6400/17241]\n",
      "loss: 0.513593  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.621663 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.319493  [    0/17241]\n",
      "loss: 0.535222  [ 6400/17241]\n",
      "loss: 0.405487  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.609126 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.624573  [    0/17241]\n",
      "loss: 0.608183  [ 6400/17241]\n",
      "loss: 0.860118  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.602000 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.569340  [    0/17241]\n",
      "loss: 0.738513  [ 6400/17241]\n",
      "loss: 0.520321  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.595639 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.674157  [    0/17241]\n",
      "loss: 0.535069  [ 6400/17241]\n",
      "loss: 0.424736  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.588028 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.617705  [    0/17241]\n",
      "loss: 0.626720  [ 6400/17241]\n",
      "loss: 0.328421  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.580828 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.532288  [    0/17241]\n",
      "loss: 0.569670  [ 6400/17241]\n",
      "loss: 0.702560  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.580943 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.568537  [    0/17241]\n",
      "loss: 0.608785  [ 6400/17241]\n",
      "loss: 0.449343  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.569347 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.451404  [    0/17241]\n",
      "loss: 0.589863  [ 6400/17241]\n",
      "loss: 0.519886  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.562514 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.715205  [    0/17241]\n",
      "loss: 0.368722  [ 6400/17241]\n",
      "loss: 0.644886  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.556992 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.555476  [    0/17241]\n",
      "loss: 0.499078  [ 6400/17241]\n",
      "loss: 0.706900  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.554169 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.456981  [    0/17241]\n",
      "loss: 0.685172  [ 6400/17241]\n",
      "loss: 0.448691  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.551921 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.670262  [    0/17241]\n",
      "loss: 0.473386  [ 6400/17241]\n",
      "loss: 0.334863  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.543548 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.594992  [    0/17241]\n",
      "loss: 0.486966  [ 6400/17241]\n",
      "loss: 0.451298  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.540653 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.643415  [    0/17241]\n",
      "loss: 0.466828  [ 6400/17241]\n",
      "loss: 0.556516  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.536866 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.588146  [    0/17241]\n",
      "loss: 0.380131  [ 6400/17241]\n",
      "loss: 0.294007  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.526614 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.451598  [    0/17241]\n",
      "loss: 0.375658  [ 6400/17241]\n",
      "loss: 0.612702  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.522528 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.443759  [    0/17241]\n",
      "loss: 0.622231  [ 6400/17241]\n",
      "loss: 0.437070  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.520506 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.612293  [    0/17241]\n",
      "loss: 0.449138  [ 6400/17241]\n",
      "loss: 0.654083  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.514788 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.352784  [    0/17241]\n",
      "loss: 0.547552  [ 6400/17241]\n",
      "loss: 0.664427  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.511129 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.515128  [    0/17241]\n",
      "loss: 0.407491  [ 6400/17241]\n",
      "loss: 0.393850  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.505919 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.375175  [    0/17241]\n",
      "loss: 0.234742  [ 6400/17241]\n",
      "loss: 0.492737  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.502235 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.547551  [    0/17241]\n",
      "loss: 0.243868  [ 6400/17241]\n",
      "loss: 0.501757  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.499726 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.428459  [    0/17241]\n",
      "loss: 0.611958  [ 6400/17241]\n",
      "loss: 0.507488  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.497624 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.552323  [    0/17241]\n",
      "loss: 0.641105  [ 6400/17241]\n",
      "loss: 0.402499  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.492534 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.420324  [    0/17241]\n",
      "loss: 0.519346  [ 6400/17241]\n",
      "loss: 0.378728  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.486664 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.398169  [    0/17241]\n",
      "loss: 0.541762  [ 6400/17241]\n",
      "loss: 0.408629  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.484006 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.351486  [    0/17241]\n",
      "loss: 0.406762  [ 6400/17241]\n",
      "loss: 0.446139  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.484145 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.316812  [    0/17241]\n",
      "loss: 0.413972  [ 6400/17241]\n",
      "loss: 0.321516  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.476858 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.343248  [    0/17241]\n",
      "loss: 0.560037  [ 6400/17241]\n",
      "loss: 0.319738  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.474089 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.380872  [    0/17241]\n",
      "loss: 0.348228  [ 6400/17241]\n",
      "loss: 0.437429  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.473439 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.478200  [    0/17241]\n",
      "loss: 0.562756  [ 6400/17241]\n",
      "loss: 0.248875  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.469497 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.388129  [    0/17241]\n",
      "loss: 0.483112  [ 6400/17241]\n",
      "loss: 0.473144  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.467588 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.570881  [    0/17241]\n",
      "loss: 0.439668  [ 6400/17241]\n",
      "loss: 0.445058  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.461910 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.353787  [    0/17241]\n",
      "loss: 0.526351  [ 6400/17241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.336177  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.459638 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.384228  [    0/17241]\n",
      "loss: 0.520854  [ 6400/17241]\n",
      "loss: 0.573929  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.457010 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.505392  [    0/17241]\n",
      "loss: 0.236574  [ 6400/17241]\n",
      "loss: 0.555562  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.456293 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.449335  [    0/17241]\n",
      "loss: 0.309241  [ 6400/17241]\n",
      "loss: 0.425991  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.454103 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.366268  [    0/17241]\n",
      "loss: 0.544982  [ 6400/17241]\n",
      "loss: 0.380674  [12800/17241]\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.451009 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
